{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyxu.abc import Func, ProxFunc, DiffFunc, LinOp\n",
    "import pyxu.operator as pxop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with pyxu stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement l2 norm (already exists in pyxu)\n",
    "class SquaredL2(Func):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return (arr**2).sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "squaredl2func = SquaredL2(dim=x.size)\n",
    "\n",
    "print(squaredl2func(x))  # no need to explicitly call apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement l2 norm with its gradient\n",
    "class SquaredL2(DiffFunc):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return (arr**2).sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def grad(self, arr):\n",
    "        return 2 * arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement l1 norm and its proximal operator\n",
    "class L1Norm(ProxFunc):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return np.abs(arr).sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def prox(self, arr, tau):\n",
    "        return np.sign(arr) * np.clip(np.abs(arr) - tau, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth out l1 norm to obtain Huber loss\n",
    "dim = 1\n",
    "huber = L1Norm(dim).moreau_envelope(mu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22be7ddcfd0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.linspace(-5, 5, 100)\n",
    "t = t.reshape(-1, 1)  # reshape for technical reasons\n",
    "\n",
    "plt.plot(t, huber(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn l2 norm into a loss function (computes distance to given vector y)\n",
    "dim = 10\n",
    "b = np.ones(dim)\n",
    "l2_loss = pxop.SquaredL2Norm(b.size).asloss(b)  # computes ||arr - b||^2\n",
    "#print(l2_loss(x)) where x is some array with dimension dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.]\n"
     ]
    }
   ],
   "source": [
    "l1_norm = pxop.L1Norm(dim=dim)\n",
    "print(l1_norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to implement likelihood (univariate HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dimensions\n",
    "M = 1  # total number of processes\n",
    "K = 10  # total number of arrivals across all processes\n",
    "\n",
    "# Define random matrix A of size (M+K) x M\n",
    "A = np.random.normal(0, 1, (K+M, M*(1+M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define E: R^{K+1} -> R, E(x) = -\\sum_{i=1}^{k} ln(x_i) + x_{K+1} as a DiffFunc\n",
    "class LikelihoodE(DiffFunc):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return -np.sum(np.log(arr[:-1])) + arr[-1]\n",
    "\n",
    "    def grad(self, arr):\n",
    "        # current gradient is wrong, need to return [-1/x1, ..., -1/xK, 1]\n",
    "        if np.any(arr[:-1] == 0):\n",
    "            print(\"Warning: divison by zero in the gradient.\")\n",
    "        return np.hstack([-1/arr[:-1], 1])\n",
    "\n",
    "# Define linear operator. TODO: maybe implement this in a matrix-free way ?\n",
    "opA = LinOp.from_array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99989632]\n",
      "0.9999999998311266\n"
     ]
    }
   ],
   "source": [
    "log = pxop.log(pxop.IdentityOp(dim=1))\n",
    "\n",
    "x = np.array([2.718])\n",
    "print(log(x))\n",
    "print(np.log(2.718281828))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "E = LikelihoodE(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.0\n",
      "[-0.36787944 -0.36787944 -0.36787944 -0.36787944 -0.36787944 -0.36787944\n",
      " -0.36787944 -0.36787944 -0.36787944  1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = np.e*np.ones(dim)\n",
    "x[-1] = 0\n",
    "print(E(x))\n",
    "print(E.grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to implement likelihood (multivariate HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dimensions\n",
    "# M = 8  # total number of processes\n",
    "# k = [100, 165, 244, 116, 255, 342, 226, 353]  # [numberOfArrivalOfN1, numberOfArrivalOfN2, ...], toy numbers\n",
    "# K = sum(k)  # total number of arrivals across all processes\n",
    "M = 2\n",
    "k = [4, 6]\n",
    "K = sum(k)\n",
    "\n",
    "# Define random matrix A of size (M+K) x M with positive entries\n",
    "A = np.random.normal(0, 1, (K+M, M*(1+M)))\n",
    "A[np.where(A<=0)] = 0.1\n",
    "\n",
    "# Define subset S of {1, ..., M+K} of row indices of A which contribute to the compensator term\n",
    "S = np.zeros(M)  # we know that |S| = M\n",
    "for i in range(M):\n",
    "    S[i] = sum(k[:(i+1)]) + i + 1\n",
    "S = [int(si)-1 for si in S]  # convert to int, decaler de -1 pour avoir tout entre 0 et M+K-1\n",
    "\n",
    "# Define the complement of S\n",
    "allIndices = list(range(0, M+K))  # or range(1, M+K+1)\n",
    "notS = list(set(allIndices) - set(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define E: R^{K+M} -> R, E(x) = -\\sum_{i \\not\\in S} ln(x_i) + \\sum_{i \\in S} as a DiffFunc\n",
    "class LikelihoodE(DiffFunc):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return sum(-np.log(arr[notS])) + sum(arr[S])\n",
    "\n",
    "    def grad(self, arr):\n",
    "        grad = np.ones(arr.shape)\n",
    "        grad[notS] = -1/arr[notS]\n",
    "        return grad\n",
    "\n",
    "# Define linear operator. TODO: maybe implement this in a matrix-free way ?\n",
    "opA = LinOp.from_array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = M+K\n",
    "E = LikelihoodE(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "[-1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones(dim)\n",
    "print(E(x))\n",
    "print(E.grad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define -log(likelihood) (to minimize) as -log(L(theta)) = E(A*theta)\n",
    "L = E * opA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.450670427977835\n",
      "[-1.38300152 -1.21361301 -0.757177   -0.33146924 -1.03682223 -1.62176489]\n"
     ]
    }
   ],
   "source": [
    "theta = np.ones(M*(1+M))\n",
    "print(L(theta))\n",
    "print(L.grad(theta))\n",
    "#print(opA.T(E.grad(opA(theta))))  # true gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
