{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pyxu.abc import DiffFunc, LinOp\n",
    "import pyxu.operator as pxop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neuron data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read neuron spike data\n",
    "data = pd.read_csv(\"simulated_data/events0.csv\", header=None)\n",
    "data = [data.iloc[i].values for i in range(len(data))]  # convert to list of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain useful constants\n",
    "M = len(data)  # number of neurons\n",
    "\n",
    "t = data  # pour faciliter les notations apres\n",
    "for i in range(M):\n",
    "    t[i] = t[i][np.nonzero(t[i])]  # get rid of arrival times = 0\n",
    "\n",
    "k = [len(ti) for ti in t]  # number of arrivals of each neuron\n",
    "T = max(max(t[i]) for i in range(M))  # TODO: Do i need T to be the last arrival, or > last arrival ?\n",
    "K = sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 165, 244, 116, 255, 342, 226, 353] number of arrivals of each neuron\n",
      "8 = M, total number of neurons\n",
      "8 should be equal to M\n",
      "1801 total number of arrivals\n"
     ]
    }
   ],
   "source": [
    "print(k, 'number of arrivals of each neuron')\n",
    "print(M, '= M, total number of neurons')\n",
    "print(len(k), 'should be equal to M')\n",
    "print(K, 'total number of arrivals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Neurons')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot spike train for all neurons\n",
    "plotspike, ax = plt.subplots(1, 1, figsize= (10,8))\n",
    "for i in range(M):\n",
    "    ax.plot(t[i], np.zeros(len(t[i]))+i, linestyle='', marker='+')\n",
    "ax.set_xlabel('Spike trains')\n",
    "ax.set_ylabel('Neurons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute A (discrete likelihood matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the HP\n",
    "beta = 1  # we choose this\n",
    "betainv = 1/beta\n",
    "def g(t, beta=beta):\n",
    "    return t * np.exp(-beta*t) * (t >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A^j for j=1,...,M\n",
    "A = [np.zeros((k[j]+1, M+1)) for j in range(M)]  # all matrices A\n",
    "\n",
    "B = np.zeros(M)  # to help compute the last row of A^j\n",
    "for n in range(M):\n",
    "    B[n] = betainv * sum((T - tnl + betainv)*np.exp(-beta*(T - tnl)) - betainv for tnl in t[n])\n",
    "\n",
    "# Fill out A^j for every j\n",
    "for j in range(M):\n",
    "    # First column computation\n",
    "    A[j][:, 0] = 1\n",
    "    A[j][-1, 0] = -T\n",
    "\n",
    "    # Fill out the rest of the matrix (-1 because last row is filled out separately)\n",
    "    for i in range(A[j].shape[0] - 1):\n",
    "        for n in range(M):\n",
    "            A[j][i, n+1] = sum(g(t[j][i] - tnl) for tnl in t[n] if tnl < t[j][i])\n",
    "\n",
    "    # Fill out last row\n",
    "    for n in range(M):\n",
    "        A[j][-1, n+1] = B[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-99.0892695334861\n"
     ]
    }
   ],
   "source": [
    "print(sum((T - tnl + betainv)*np.exp(-beta*(T - tnl)) - betainv for tnl in t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot matrix A (without last row, to keep things to scale)\n",
    "j = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,8))\n",
    "im = ax.imshow(A[j][0:-1, :], cmap='viridis', interpolation='nearest', extent=[1, M, 0, k[j]/5])\n",
    "fig.colorbar(im, ax=ax)  # Add a colorbar to show the scale\n",
    "ax.set_title('Matrix Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define -log(likelihood) operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> Recall that we want to minimize $-\\ln(L(\\theta)) = E(A\\theta)$ where: \n",
    "- $E: \\mathbb{R}^{M+K} \\to \\mathbb{R}$, $x \\mapsto E(x) = -\\sum_{x \\not\\in S} \\ln(x_i) + \\sum_{x \\in S} x_i$, $S \\subset \\{1, ..., M+K\\}$ the set of indices contributing to the compensator term\n",
    "- $A \\in \\mathbb{R}^{(M+K) \\times (M + M^2)}$ is the discrete likelihood matrix\n",
    "- $\\theta \\in \\mathbb{R}^{M + M^2}$ is the vector of parameters, $\\theta = (\\mu_1, \\vec{\\alpha}_1, ..., \\mu_M, \\vec{\\alpha}_M)$ \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Toy dimensions\n",
    "# M = 1  # total number of processes\n",
    "# K = 10  # total number of arrivals across all processes\n",
    "\n",
    "# # Define random matrix A of size (M+K) x M\n",
    "# A = np.random.normal(0, 1, (K+M, M*(1+M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define each A^j as an operator\n",
    "ops = [None for _ in range(M)]\n",
    "\n",
    "for i in range(M):\n",
    "    ops[i] = LinOp.from_array(A[i])\n",
    "\n",
    "# Define A of shape (K+M, M*(1+M)) as block-diagonal. TODO: maybe implement this in a matrix free way ?\n",
    "opA = pxop.block_diag(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset S of {1, ..., M+K} of row indices of A which contribute to the compensator term\n",
    "S = np.zeros(M)  # we know that |S| = M\n",
    "for i in range(M):\n",
    "    S[i] = sum(k[:(i+1)]) + i + 1\n",
    "S = [int(si)-1 for si in S]  # convert to int, decaler de -1 pour avoir tout entre 0 et M+K-1\n",
    "\n",
    "# Define the complement of S\n",
    "allIndices = list(range(0, M+K))  # or range(1, M+K+1)\n",
    "notS = list(set(allIndices) - set(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define E: R^{K+M} -> R, E(x) = -\\sum_{i \\not\\in S} ln(x_i) + \\sum_{i \\in S} as a DiffFunc\n",
    "class LikelihoodE(DiffFunc):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(shape=(1, dim))\n",
    "\n",
    "    def apply(self, arr):\n",
    "        return sum(-np.log(arr[notS])) + sum(arr[S])\n",
    "\n",
    "    def grad(self, arr):\n",
    "        grad = np.ones(arr.shape)\n",
    "        grad[notS] = -1/arr[notS]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "[-1. -1. -1. ... -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "dim = M+K\n",
    "E = LikelihoodE(dim=dim)\n",
    "\n",
    "x = np.ones(dim)\n",
    "print(E(x))\n",
    "print(E.grad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define -log(likelihood) (to minimize) as -log(L(theta)) = E(A*theta)\n",
    "L = E * opA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood: -20217.70024678208\n"
     ]
    }
   ],
   "source": [
    "theta = np.ones(M*(1+M))\n",
    "print('likelihood:', L(theta))\n",
    "#print('gradient:', L.grad(theta))\n",
    "#print('gradient computed by hand:', opA.T(E.grad(opA(theta))))  # true gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with everything in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hawkes_likelihood import HawkesLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"simulated_data/events0.csv\"\n",
    "hpL = HawkesLikelihood(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpL.plot_realization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "hpL.plot_A(idx=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1809, 72)\n",
      "(1, 72)\n"
     ]
    }
   ],
   "source": [
    "print(hpL.opA.shape)  # = (K+M, M^2 + M)\n",
    "print(hpL.negLogL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "[-1. -1. -1. ... -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "dim = hpL.M + hpL.K  # = hpL.opA.shape[0]\n",
    "\n",
    "x = np.ones(dim)\n",
    "print(hpL.E(x))\n",
    "print(hpL.E.grad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood: -20217.70024678208\n"
     ]
    }
   ],
   "source": [
    "theta = np.ones(hpL.negLogL.shape[1])  # = M^2 + M\n",
    "print('likelihood:', hpL.negLogL(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 100 100 100 100 100 100 100 100]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(hpL.A[0] < 0)[0])\n",
    "#print(hpL.A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99999999e-01 1.00000000e+09 9.99999999e-01]\n"
     ]
    }
   ],
   "source": [
    "ez = np.array([1, 0, 1])\n",
    "print(1/(ez+10e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
